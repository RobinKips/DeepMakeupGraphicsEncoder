# Deep Graphics Encoder for Real-Time Video Makeup Synthesis from Example
<!--
<div>
    <h2><a style="width: 20%;margin: 2.5%;" href="https://www.linkedin.com/in/robin-kips" target="_blank">Robin Kips</a><a style="width: 20%;margin: 2.5%;" href="https://www.linkedin.com/in/pietro-gori-b097bb118/" target="_blank">Pietro Gori</a><a style="width: 20%;margin: 2.5%;" href="https://www.linkedin.com/in/matthieu-perrot-225ab01b/" target="_blank">Matthieu Perrot</a><a style="width: 20%;margin: 2.5%;" href="https://www.linkedin.com/in/isabelle-bloch-b954144/" target="_blank">Isabelle Bloch</a></h2>
</div>


<p align="center">
   <img  style="width: 32%;margin: 2.5%;" width="40%" src="images/loreal_research.png">  <img  style="width: 12%;margin: 2.5%;" width="40%" src="images/telecom_icon.png">   <img  style="width: 34%;margin: 2.5%;" width="40%" src="images/idp_icon.png">
</p>



![example_style_transfer](images/full_face_shades.png)
-->

### Abstract:
While makeup virtual-try-on is now widespread, parametrizing a computer graphics rendering engine for synthesizing images of a given cosmetics product remains a challenging task.
In this paper, we introduce an inverse computer graphics method for automatic makeup synthesis from a reference image, by learning a model that maps an example portrait image with makeup to the space of rendering parameters. 
This method can be used by artists to automatically create realistic virtual cosmetics image samples, or by consumers, to virtually try-on a makeup extracted from their favourite reference image.


### Video examples:

<iframe  style="display: block; margin: auto;" width="560" height="315" src="https://www.youtube.com/embed/GmciY9rUMOw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe style="display: block; margin: auto;"  width="560" height="315" src="https://www.youtube.com/embed/0dMrf0yZvUw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### Paper:

Work in Progress
<!--
Paper : [ECCV Workshop proceedings](https://link.springer.com/chapter/10.1007%2F978-3-030-67070-2_17) \
ArXiv : [https://arxiv.org/abs/2008.10298](https://arxiv.org/abs/2008.10298) \
Supplementary Materials : [ECCV Workshop supplementary](https://static-content.springer.com/esm/chp%3A10.1007%2F978-3-030-67070-2_17/MediaObjects/509619_1_En_17_MOESM6_ESM.pdf)
 <div align="center" style="display:flex; margin-bottom:50px; margin-top: 30px;">
    <div style="width:20%;display: inline-block;">    
        <a href="https://arxiv.org/abs/2008.10298" target="_blank">
            <img class="layered-paper-big" style="max-height:200px" src="images/ca_gan_paper-page-001.jpg">
        </a>
    </div>
    <div style="width:70%;display: flex; align-items: center; margin-left: 5%;">
        <div style="text-align: left;">
            <span style="font-size:12pt">R. Kips, P. Gori, M. Perrot, I.Bloch</span><br>
            <span style="font-size:12pt">
                <b>CA-GAN: Weakly Supervised Color Aware GAN for Controllable Makeup Transfer</b>
            </span>
            <br>
            <span style="font-size:12pt">AIM20 (ECCV20 Workshop)</span>
            <span style="font-size:12pt"><a href="https://arxiv.org/abs/2008.10298" target="_blank">[arXiv]</a>&nbsp;<a href="bibtex.txt" target="_blank">[BibTeX]</a>&nbsp;<a href="ca_gan_supplementary.pdf" target="_blank">[Supplementary Materials]</a></span>
        </div>
    </div>
</div>
-->

